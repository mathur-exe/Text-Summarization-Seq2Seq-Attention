{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization (Seq2Seq).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IR5wMVmT4RSM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtTOzIV0u5kp"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import io\n",
        "import json\n",
        "from keras_preprocessing.text import tokenizer_from_json\n",
        "import datetime\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VjrwS-yAxqmC",
        "outputId": "9180889e-686e-42ae-bf9c-e0501b9b156d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkaFlwnSxrYU",
        "outputId": "0a83f1e5-8be5-43df-d684-ca0c2244a91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.MirroredStrategy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iiaKVZxxuJu",
        "outputId": "c9095da1-f341-462f-eeac-9833d3b83e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/CSE Research papers/Text Summarization /Reviews.csv')\n",
        "train = train[['Summary','Text']]\n",
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8jCs5rxjxwKC",
        "outputId": "eceb5ffa-e5d7-4b0f-ac6a-4facb613715a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Summary                                               Text\n",
              "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
              "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
              "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
              "4            Great taffy  Great taffy at a great price.  There was a wid..."
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['text_length'] = train['Text'].str.count(' ')\n",
        "train['text_length'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2QMF9mNx1xn",
        "outputId": "4ca4f218-9be2-4dc9-b58c-a25cdd2932f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    568454.000000\n",
              "mean         81.005522\n",
              "std          80.807102\n",
              "min           2.000000\n",
              "25%          33.000000\n",
              "50%          57.000000\n",
              "75%          99.000000\n",
              "max        3525.000000\n",
              "Name: text_length, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['summary_length'] = train['Summary'].str.count(' ')\n",
        "train['summary_length'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSqL0Xuzx4tE",
        "outputId": "e0f4e54e-494f-4dff-d562-7daa6aa215e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    568427.000000\n",
              "mean          3.128462\n",
              "std           2.619420\n",
              "min           0.000000\n",
              "25%           1.000000\n",
              "50%           3.000000\n",
              "75%           4.000000\n",
              "max          41.000000\n",
              "Name: summary_length, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "YOdB2Y5V3PX3",
        "outputId": "1fe8f0e6-00a8-43b4-c471-768c3f74847c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>text_length</th>\n",
              "      <th>summary_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>48</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "      <td>30</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "      <td>98</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "      <td>42</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "      <td>29</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Summary  ... summary_length\n",
              "0  Good Quality Dog Food  ...            3.0\n",
              "1      Not as Advertised  ...            2.0\n",
              "2  \"Delight\" says it all  ...            3.0\n",
              "3         Cough Medicine  ...            1.0\n",
              "4            Great taffy  ...            1.0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[train['summary_length']>=2].reset_index(drop=True)\n",
        "train = train[train['summary_length']<=20].reset_index(drop=True)\n",
        "train = train[train['text_length']<=100].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "SQKquqER3VQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "print(train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81a-zdjf3e3W",
        "outputId": "06d4c032-e86f-421d-86bb-7deb5d3bcd0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(276719, 4)\n",
            "                                         Summary  ... summary_length\n",
            "0                          Good Quality Dog Food  ...            3.0\n",
            "1                              Not as Advertised  ...            2.0\n",
            "2                          \"Delight\" says it all  ...            3.0\n",
            "3  Great!  Just as good as the expensive brands!  ...            8.0\n",
            "4                         Wonderful, tasty taffy  ...            2.0\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['text_lower'] = train['Text'].str.lower()\n",
        "train['text_no_punctuation'] = train['text_lower'].str.replace('[^\\w\\s]','')"
      ],
      "metadata": {
        "id": "Hk9iKRO93gjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['summary_lower'] = train[\"Summary\"].str.lower()\n",
        "train['summary_no_punctuation'] =  '_start_' + ' ' +train['summary_lower'].str.replace('[^\\w\\s]','')+ ' ' +'_end_'"
      ],
      "metadata": {
        "id": "iKUhWtr33oFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "#save 100 values as a test dataset for scoring\n",
        "\n",
        "test = train[0:100]\n",
        "train = train[100:]\n",
        "test.to_csv('test_set.csv')"
      ],
      "metadata": {
        "id": "kYKkhu7r3s0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features1 = 100000\n",
        "maxlen1 = 100\n",
        "\n",
        "max_features2 = 100000\n",
        "maxlen2 = 20"
      ],
      "metadata": {
        "id": "-S76Z_jT3wNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok1 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features1) \n",
        "tok1.fit_on_texts(list(train['text_no_punctuation'].astype(str))) #fit to cleaned text\n",
        "tf_train_text =tok1.texts_to_sequences(list(train['text_no_punctuation'].astype(str)))\n",
        "tf_train_text =tf.keras.preprocessing.sequence.pad_sequences(tf_train_text, maxlen=maxlen1) #let's execute pad step "
      ],
      "metadata": {
        "id": "T0O06uxH3yoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1_json = tok1.to_json()\n",
        "with io.open('tok1.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(tokenizer1_json, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "Sbe9f_MJ30ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok2 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features2, filters = '*') \n",
        "tok2.fit_on_texts(list(train['summary_no_punctuation'].astype(str))) #fit to cleaned text\n",
        "tf_train_summary = tok2.texts_to_sequences(list(train['summary_no_punctuation'].astype(str)))\n",
        "tf_train_summary = tf.keras.preprocessing.sequence.pad_sequences(tf_train_summary, maxlen=maxlen2, padding ='post')"
      ],
      "metadata": {
        "id": "mbfRmYRn39Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2_json = tok2.to_json()\n",
        "with io.open('tok2.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(tokenizer2_json, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "jY_69Wob4DuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_summary = tf_train_summary\n",
        "# For Decoder Input, you don't need the last word as that is only for prediction\n",
        "# when we are training using Teacher Forcing.\n",
        "decoder_input_data = vectorized_summary[:, :-1]\n",
        "\n",
        "# Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n",
        "decoder_target_data = vectorized_summary[:, 1:]\n",
        "\n",
        "print(f'Shape of decoder input: {decoder_input_data.shape}')\n",
        "print(f'Shape of decoder target: {decoder_target_data.shape}')\n",
        "\n",
        "vectorized_text = tf_train_text\n",
        "# Encoder input is simply the body of the text\n",
        "encoder_input_data = vectorized_text\n",
        "doc_length = encoder_input_data.shape[1]\n",
        "print(f'Shape of encoder input: {encoder_input_data.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7K0ElPy4FUD",
        "outputId": "ddb232c6-c399-4fbd-fa45-5f037575d985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of decoder input: (276619, 19)\n",
            "Shape of decoder target: (276619, 19)\n",
            "Shape of encoder input: (276619, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setting size of vocabulary encoder and decoder\n",
        "vocab_size_encoder = len(tok1.word_index) + 1 \n",
        "vocab_size_decoder = len(tok2.word_index) + 1"
      ],
      "metadata": {
        "id": "a3fWTh8Q4He6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set latent dimension for embedding and hidden units\n",
        "latent_dim = 100"
      ],
      "metadata": {
        "id": "xobCVkIF4JTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GLOVE_DIR = \"/home/tiana/Data_Science/tests/glove\"\n",
        "\n",
        "# embeddings_index = {}\n",
        "# f = open(os.path.join(GLOVE_DIR, 'glove.6B.{}d.txt'.format(latent_dim)))\n",
        "# for line in f:\n",
        "#     values = line.split()\n",
        "#     word = values[0]\n",
        "#     coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "\n",
        "# print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "JiTXFC3M4P3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build embedding weights matrix for text\n",
        "\n",
        "embedding_matrix = np.zeros((len(tok1.word_index) + 1, latent_dim))\n",
        "for word, i in tok1.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "0h2dPfcv4Wlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "#### Encoder Model ####\n",
        "#setting Encoder Input\n",
        "#putting the model under\n",
        "#our GPU strategy\n",
        "\n",
        "\n",
        "with strategy.scope():    \n",
        "    encoder_inputs = tf.keras.Input(shape=(doc_length,), name='Encoder-Input')\n",
        "\n",
        "        # GloVe Embeding for encoder\n",
        "    x = tf.keras.layers.Embedding(vocab_size_encoder, \n",
        "                                  latent_dim, \n",
        "                                  name='Body-Word-Embedding',\n",
        "                                      weights=[embedding_matrix],\n",
        "                                      mask_zero=False, \n",
        "                                      trainable=False)(encoder_inputs)\n",
        "\n",
        "        #Batch normalization is used so that the distribution of the inputs \n",
        "        #to a specific layer doesn't change over time\n",
        "    x = tf.keras.layers.BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
        "\n",
        "\n",
        "        # We do not need the `encoder_output` just the hidden state\n",
        "    _, state_h = tf.keras.layers.GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
        "\n",
        "        # Set the encoder as a separate entity so we can encode without decoding if desired\n",
        "    encoder_model = tf.keras.Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
        "\n",
        "\n",
        "    seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
        "\n",
        "\n",
        "\n",
        "        ########################\n",
        "        #### Decoder Model ####\n",
        "    decoder_inputs = tf.keras.Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
        "\n",
        "        # Embedding For Decoder, not GloVe \n",
        "    dec_emb = tf.keras.layers.Embedding(vocab_size_decoder, \n",
        "                                            latent_dim, \n",
        "                                            name='Decoder-Word-Embedding',\n",
        "                                            mask_zero=False, )(decoder_inputs)\n",
        "\n",
        "        #batch normalization\n",
        "    dec_bn = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
        "\n",
        "        # Set up the decoder, using `decoder_state_input` as initial state.\n",
        "    decoder_gru = tf.keras.layers.GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
        "        #the decoder \"decodes\" the encoder out\n",
        "    decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
        "    x = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
        "\n",
        "        # Dense layer for prediction\n",
        "    decoder_dense = tf.keras.layers.Dense(vocab_size_decoder, activation='softmax', name='Final-Output-Dense')\n",
        "    decoder_outputs = decoder_dense(x)\n",
        "\n",
        "\n",
        "        ########################\n",
        "        #### Seq2Seq Model ####\n",
        "    seq2seq_Model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    seq2seq_Model.compile(optimizer=tf.keras.optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_g8Ov4l6YCb",
        "outputId": "b16cd1f3-03ea-4053-e644-16576f2d54e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/nadam.py:73: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Nadam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from seq2seq_utils import viz_model_architecture\n",
        "seq2seq_Model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsTkWo1w6Zxn",
        "outputId": "2c0c0ffc-c182-4709-f91e-1dcfbf810cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Decoder-Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Decoder-Word-Embedding (Embedd  (None, None, 100)   2876100     ['Decoder-Input[0][0]']          \n",
            " ing)                                                                                             \n",
            "                                                                                                  \n",
            " Encoder-Input (InputLayer)     [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " Decoder-Batchnorm-1 (BatchNorm  (None, None, 100)   400         ['Decoder-Word-Embedding[0][0]'] \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-Model (Functional)     (None, 100)          11201600    ['Encoder-Input[0][0]']          \n",
            "                                                                                                  \n",
            " Decoder-GRU (GRU)              [(None, None, 100),  60600       ['Decoder-Batchnorm-1[0][0]',    \n",
            "                                 (None, 100)]                     'Encoder-Model[0][0]']          \n",
            "                                                                                                  \n",
            " Decoder-Batchnorm-2 (BatchNorm  (None, None, 100)   400         ['Decoder-GRU[0][0]']            \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Final-Output-Dense (Dense)     (None, None, 28761)  2904861     ['Decoder-Batchnorm-2[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 17,043,961\n",
            "Trainable params: 5,902,761\n",
            "Non-trainable params: 11,141,200\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find values for train/val split \n",
        "#I'm choosing a 85/15 train/val split\n",
        "\n",
        "data_len = len(encoder_input_data)\n",
        "val_split = int(np.floor(data_len*.15))\n",
        "train_split = int(np.floor(data_len*.85))\n",
        "\n",
        "\n",
        "#set hyperparameters\n",
        "train_buffer_size = train_split\n",
        "val_buffer_size = val_split\n",
        "\n",
        "batch_size_per_replica = 256\n",
        "global_batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n",
        "\n",
        "\n",
        "# separating into train and validation data\n",
        "X_enc_train = encoder_input_data[0:train_split]\n",
        "X_dec_train = decoder_input_data[0:train_split]\n",
        "y_t_train = np.expand_dims(decoder_target_data, -1)[0:train_split]\n",
        "\n",
        "X_enc_val = encoder_input_data[-val_split:-1]\n",
        "X_dec_val = decoder_input_data[-val_split:-1]\n",
        "y_t_val = np.expand_dims(decoder_target_data, -1)[-val_split:-1]\n",
        "\n",
        "\n",
        "#tf.data - make dataset, shuffle, batch and prefetch it\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(({\"Encoder-Input\": X_enc_train, \"Decoder-Input\": X_dec_train}, y_t_train))\n",
        "train_dataset = train_dataset.shuffle(train_buffer_size)\n",
        "train_dataset = train_dataset.batch(global_batch_size).prefetch(1) \n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(({\"Encoder-Input\": X_enc_val, \"Decoder-Input\": X_dec_val}, y_t_val))\n",
        "val_dataset = val_dataset.shuffle(val_buffer_size)\n",
        "val_dataset = val_dataset.batch(global_batch_size).prefetch(1) "
      ],
      "metadata": {
        "id": "UyypkNy46d1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorboard\n",
        "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "#checkpoints\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='/tmp/weights.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "#early_stopping\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',  \n",
        "                                              patience=10, \n",
        "                                              verbose=1, mode='auto', \n",
        "                                              restore_best_weights=True)\n",
        "\n",
        "#model\n",
        "epochs = 2\n",
        "history = seq2seq_Model.fit(train_dataset, validation_data = val_dataset,\n",
        "                            epochs=epochs,  \n",
        "                            validation_steps = val_split // global_batch_size,\n",
        "                            callbacks=[tensorboard_callback, checkpointer]) \n",
        "\n",
        "\n",
        "#save final model\n",
        "seq2seq_Model.save('your_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXzLu-Um6eyB",
        "outputId": "97df34bd-ae81-42d0-8e89-c48019d97b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "919/919 [==============================] - ETA: 0s - loss: 1.9401\n",
            "Epoch 00001: val_loss improved from inf to 1.33148, saving model to /tmp/weights.01-1.33.hdf5\n",
            "919/919 [==============================] - 181s 188ms/step - loss: 1.9401 - val_loss: 1.3315\n",
            "Epoch 2/2\n",
            "919/919 [==============================] - ETA: 0s - loss: 1.2275\n",
            "Epoch 00002: val_loss improved from 1.33148 to 1.22456, saving model to /tmp/weights.02-1.22.hdf5\n",
            "919/919 [==============================] - 171s 185ms/step - loss: 1.2275 - val_loss: 1.2246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_Model.save('your_model.h5')"
      ],
      "metadata": {
        "id": "o0IpzDMl6jgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the model\n",
        "seq2seq_Model = tf.keras.models.load_model('your_model.h5')\n",
        "\n",
        "# Show the model architecture\n",
        "seq2seq_Model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxOMbdUQ6kas",
        "outputId": "a35f3e77-c953-43dd-abeb-4862fd9c5f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Decoder-Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Decoder-Word-Embedding (Embedd  (None, None, 100)   2876100     ['Decoder-Input[0][0]']          \n",
            " ing)                                                                                             \n",
            "                                                                                                  \n",
            " Encoder-Input (InputLayer)     [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " Decoder-Batchnorm-1 (BatchNorm  (None, None, 100)   400         ['Decoder-Word-Embedding[0][0]'] \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Encoder-Model (Functional)     (None, 100)          11201600    ['Encoder-Input[0][0]']          \n",
            "                                                                                                  \n",
            " Decoder-GRU (GRU)              [(None, None, 100),  60600       ['Decoder-Batchnorm-1[0][0]',    \n",
            "                                 (None, 100)]                     'Encoder-Model[0][0]']          \n",
            "                                                                                                  \n",
            " Decoder-Batchnorm-2 (BatchNorm  (None, None, 100)   400         ['Decoder-GRU[0][0]']            \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Final-Output-Dense (Dense)     (None, None, 28761)  2904861     ['Decoder-Batchnorm-2[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 17,043,961\n",
            "Trainable params: 5,902,761\n",
            "Non-trainable params: 11,141,200\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#open the tokenizers\n",
        "\n",
        "with open('tok1.json') as f:\n",
        "    data = json.load(f)\n",
        "    tok1 = tokenizer_from_json(data)\n",
        "    \n",
        "with open('tok2.json') as f:\n",
        "    data = json.load(f)\n",
        "    tok2 = tokenizer_from_json(data)"
      ],
      "metadata": {
        "id": "i7QVdk_z6nGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#look at test set\n",
        "test.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I0VnFbw_6p5V",
        "outputId": "ee9e0235-e683-47ba-e523-d7edc82b2990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>text_length</th>\n",
              "      <th>summary_length</th>\n",
              "      <th>text_lower</th>\n",
              "      <th>text_no_punctuation</th>\n",
              "      <th>summary_lower</th>\n",
              "      <th>summary_no_punctuation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Great Roast good value</td>\n",
              "      <td>I discovered this coffee on a salmon fishing t...</td>\n",
              "      <td>50</td>\n",
              "      <td>3.0</td>\n",
              "      <td>i discovered this coffee on a salmon fishing t...</td>\n",
              "      <td>i discovered this coffee on a salmon fishing t...</td>\n",
              "      <td>great roast good value</td>\n",
              "      <td>_start_ great roast good value _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>On my list</td>\n",
              "      <td>There are a few coffees I order over and over,...</td>\n",
              "      <td>47</td>\n",
              "      <td>2.0</td>\n",
              "      <td>there are a few coffees i order over and over,...</td>\n",
              "      <td>there are a few coffees i order over and over ...</td>\n",
              "      <td>on my list</td>\n",
              "      <td>_start_ on my list _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Love, Love, LOVE!</td>\n",
              "      <td>So far my absolute favorite chips.  I get my \"...</td>\n",
              "      <td>57</td>\n",
              "      <td>2.0</td>\n",
              "      <td>so far my absolute favorite chips.  i get my \"...</td>\n",
              "      <td>so far my absolute favorite chips  i get my ch...</td>\n",
              "      <td>love, love, love!</td>\n",
              "      <td>_start_ love love love _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Love the Flavor of this tea!</td>\n",
              "      <td>I bought this tea and both my best friend and ...</td>\n",
              "      <td>33</td>\n",
              "      <td>5.0</td>\n",
              "      <td>i bought this tea and both my best friend and ...</td>\n",
              "      <td>i bought this tea and both my best friend and ...</td>\n",
              "      <td>love the flavor of this tea!</td>\n",
              "      <td>_start_ love the flavor of this tea _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great Product - Great Price!</td>\n",
              "      <td>Being gluten intolerant, this is the best all-...</td>\n",
              "      <td>27</td>\n",
              "      <td>4.0</td>\n",
              "      <td>being gluten intolerant, this is the best all-...</td>\n",
              "      <td>being gluten intolerant this is the best allpu...</td>\n",
              "      <td>great product - great price!</td>\n",
              "      <td>_start_ great product  great price _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Great Dog Food</td>\n",
              "      <td>Works to keep your dogs teeth clean and add mu...</td>\n",
              "      <td>23</td>\n",
              "      <td>2.0</td>\n",
              "      <td>works to keep your dogs teeth clean and add mu...</td>\n",
              "      <td>works to keep your dogs teeth clean and add mu...</td>\n",
              "      <td>great dog food</td>\n",
              "      <td>_start_ great dog food _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Fantastic and Addictive!</td>\n",
              "      <td>This granola bar is sweet and crunchy but does...</td>\n",
              "      <td>25</td>\n",
              "      <td>2.0</td>\n",
              "      <td>this granola bar is sweet and crunchy but does...</td>\n",
              "      <td>this granola bar is sweet and crunchy but does...</td>\n",
              "      <td>fantastic and addictive!</td>\n",
              "      <td>_start_ fantastic and addictive _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Disappointed with Amazon's policy</td>\n",
              "      <td>Price dropped by $5 in matter of 2 days.  Item...</td>\n",
              "      <td>28</td>\n",
              "      <td>3.0</td>\n",
              "      <td>price dropped by $5 in matter of 2 days.  item...</td>\n",
              "      <td>price dropped by 5 in matter of 2 days  item d...</td>\n",
              "      <td>disappointed with amazon's policy</td>\n",
              "      <td>_start_ disappointed with amazons policy _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Moist &amp; Delicious Bran Muffins</td>\n",
              "      <td>Honey Raisin Bran Muffin Mix by Sun-Maid makes...</td>\n",
              "      <td>85</td>\n",
              "      <td>4.0</td>\n",
              "      <td>honey raisin bran muffin mix by sun-maid makes...</td>\n",
              "      <td>honey raisin bran muffin mix by sunmaid makes ...</td>\n",
              "      <td>moist &amp; delicious bran muffins</td>\n",
              "      <td>_start_ moist  delicious bran muffins _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Good coffee in K Cups</td>\n",
              "      <td>The only reason I did not give it five stars i...</td>\n",
              "      <td>49</td>\n",
              "      <td>4.0</td>\n",
              "      <td>the only reason i did not give it five stars i...</td>\n",
              "      <td>the only reason i did not give it five stars i...</td>\n",
              "      <td>good coffee in k cups</td>\n",
              "      <td>_start_ good coffee in k cups _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>LOVE this coffee!</td>\n",
              "      <td>Have used both the Vanilla Nut and the Mocha J...</td>\n",
              "      <td>57</td>\n",
              "      <td>2.0</td>\n",
              "      <td>have used both the vanilla nut and the mocha j...</td>\n",
              "      <td>have used both the vanilla nut and the mocha j...</td>\n",
              "      <td>love this coffee!</td>\n",
              "      <td>_start_ love this coffee _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>DONE WiTH DIAMOND!</td>\n",
              "      <td>Another recall? I will never buy their crappy ...</td>\n",
              "      <td>20</td>\n",
              "      <td>2.0</td>\n",
              "      <td>another recall? i will never buy their crappy ...</td>\n",
              "      <td>another recall i will never buy their crappy f...</td>\n",
              "      <td>done with diamond!</td>\n",
              "      <td>_start_ done with diamond _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>250% Price increase!</td>\n",
              "      <td>Six weeks ago I bought this for $19.99, now it...</td>\n",
              "      <td>21</td>\n",
              "      <td>2.0</td>\n",
              "      <td>six weeks ago i bought this for $19.99, now it...</td>\n",
              "      <td>six weeks ago i bought this for 1999 now it is...</td>\n",
              "      <td>250% price increase!</td>\n",
              "      <td>_start_ 250 price increase _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>The dog loves it</td>\n",
              "      <td>My Golden Retriever is a heavy duty chewer.  W...</td>\n",
              "      <td>44</td>\n",
              "      <td>3.0</td>\n",
              "      <td>my golden retriever is a heavy duty chewer.  w...</td>\n",
              "      <td>my golden retriever is a heavy duty chewer  we...</td>\n",
              "      <td>the dog loves it</td>\n",
              "      <td>_start_ the dog loves it _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Don't eat a whole one</td>\n",
              "      <td>One of the only things my mini poodle gets to ...</td>\n",
              "      <td>93</td>\n",
              "      <td>4.0</td>\n",
              "      <td>one of the only things my mini poodle gets to ...</td>\n",
              "      <td>one of the only things my mini poodle gets to ...</td>\n",
              "      <td>don't eat a whole one</td>\n",
              "      <td>_start_ dont eat a whole one _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>NOT A GOOD TEXTURE - DON'T CARE FOR THESE</td>\n",
              "      <td>DON'T LIKE THE TEXTURE OF THESE SMOKED OYSTERS...</td>\n",
              "      <td>36</td>\n",
              "      <td>8.0</td>\n",
              "      <td>don't like the texture of these smoked oysters...</td>\n",
              "      <td>dont like the texture of these smoked oysters ...</td>\n",
              "      <td>not a good texture - don't care for these</td>\n",
              "      <td>_start_ not a good texture  dont care for thes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Halvah.  Great stuff.</td>\n",
              "      <td>Halvah is an acquired taste, but once you've a...</td>\n",
              "      <td>16</td>\n",
              "      <td>3.0</td>\n",
              "      <td>halvah is an acquired taste, but once you've a...</td>\n",
              "      <td>halvah is an acquired taste but once youve acq...</td>\n",
              "      <td>halvah.  great stuff.</td>\n",
              "      <td>_start_ halvah  great stuff _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Really great sauce</td>\n",
              "      <td>Tasty, easy to use, ingredient list full of ac...</td>\n",
              "      <td>33</td>\n",
              "      <td>2.0</td>\n",
              "      <td>tasty, easy to use, ingredient list full of ac...</td>\n",
              "      <td>tasty easy to use ingredient list full of actu...</td>\n",
              "      <td>really great sauce</td>\n",
              "      <td>_start_ really great sauce _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Very good, but very addictive</td>\n",
              "      <td>One pound goes a long way.  The flavor is very...</td>\n",
              "      <td>72</td>\n",
              "      <td>4.0</td>\n",
              "      <td>one pound goes a long way.  the flavor is very...</td>\n",
              "      <td>one pound goes a long way  the flavor is very ...</td>\n",
              "      <td>very good, but very addictive</td>\n",
              "      <td>_start_ very good but very addictive _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Nice for a snack or dessert...</td>\n",
              "      <td>Best when used with a cheese slicer- you can u...</td>\n",
              "      <td>76</td>\n",
              "      <td>5.0</td>\n",
              "      <td>best when used with a cheese slicer- you can u...</td>\n",
              "      <td>best when used with a cheese slicer you can us...</td>\n",
              "      <td>nice for a snack or dessert...</td>\n",
              "      <td>_start_ nice for a snack or dessert _end_</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      Summary  ...                             summary_no_punctuation\n",
              "0                      Great Roast good value  ...               _start_ great roast good value _end_\n",
              "1                                  On my list  ...                           _start_ on my list _end_\n",
              "2                           Love, Love, LOVE!  ...                       _start_ love love love _end_\n",
              "3                Love the Flavor of this tea!  ...          _start_ love the flavor of this tea _end_\n",
              "4                Great Product - Great Price!  ...           _start_ great product  great price _end_\n",
              "5                              Great Dog Food  ...                       _start_ great dog food _end_\n",
              "6                    Fantastic and Addictive!  ...              _start_ fantastic and addictive _end_\n",
              "7           Disappointed with Amazon's policy  ...     _start_ disappointed with amazons policy _end_\n",
              "8              Moist & Delicious Bran Muffins  ...        _start_ moist  delicious bran muffins _end_\n",
              "9                       Good coffee in K Cups  ...                _start_ good coffee in k cups _end_\n",
              "10                          LOVE this coffee!  ...                     _start_ love this coffee _end_\n",
              "11                         DONE WiTH DIAMOND!  ...                    _start_ done with diamond _end_\n",
              "12                       250% Price increase!  ...                   _start_ 250 price increase _end_\n",
              "13                           The dog loves it  ...                     _start_ the dog loves it _end_\n",
              "14                      Don't eat a whole one  ...                 _start_ dont eat a whole one _end_\n",
              "15  NOT A GOOD TEXTURE - DON'T CARE FOR THESE  ...  _start_ not a good texture  dont care for thes...\n",
              "16                      Halvah.  Great stuff.  ...                  _start_ halvah  great stuff _end_\n",
              "17                         Really great sauce  ...                   _start_ really great sauce _end_\n",
              "18              Very good, but very addictive  ...         _start_ very good but very addictive _end_\n",
              "19             Nice for a snack or dessert...  ...          _start_ nice for a snack or dessert _end_\n",
              "\n",
              "[20 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pick a cell from the clean data to test and look at it\n",
        "test_text = [test['text_no_punctuation'][34]]\n",
        "test_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYPhwS106sTH",
        "outputId": "3a6808c0-ff1b-446f-f233-0e5512fb7851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i have been using this curry paste for several years now and it makes a great dish although it doesnt beat eating indian out its the best paste i have used for at home dishes even though this is a hot paste i would consider it mild']"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the encoder's features for the decoder\n",
        "\n",
        "tok1.fit_on_texts(test_text)"
      ],
      "metadata": {
        "id": "aWMYZ0ZNLExZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize test text\n",
        "raw_tokenized = tok1.texts_to_sequences(test_text)\n",
        "raw_tokenized = tf.keras.preprocessing.sequence.pad_sequences(raw_tokenized, maxlen=maxlen1)"
      ],
      "metadata": {
        "id": "l5QodI2b6ugu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "print(test['summary_no_punctuation'][34])"
      ],
      "metadata": {
        "id": "9btmh9A99rF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predict the encoder state of the new sentence\n",
        "body_encoding = encoder_model.predict(raw_tokenized) "
      ],
      "metadata": {
        "id": "CJJrdqfNOEfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get output shapes of decoder word embedding\n",
        "latent_dim = seq2seq_Model.get_layer('Decoder-Word-Embedding').output_shape[-1]\n"
      ],
      "metadata": {
        "id": "7cVhN7xvOINi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get layer method for getting the embedding (word clusters)\n",
        "\n",
        "decoder_inputs = seq2seq_Model.get_layer('Decoder-Input').input \n",
        "dec_emb = seq2seq_Model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
        "dec_bn = seq2seq_Model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
        "\n",
        "gru_inference_state_input = tf.keras.Input(shape=(latent_dim,), name='hidden_state_input')\n",
        "\n",
        "gru_out, gru_state_out = seq2seq_Model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n",
        "\n",
        "# Reconstruct dense layers\n",
        "dec_bn2 = seq2seq_Model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
        "dense_out = seq2seq_Model.get_layer('Final-Output-Dense')(dec_bn2)"
      ],
      "metadata": {
        "id": "WjeB9fGjOIFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_model = tf.keras.Model([decoder_inputs, gru_inference_state_input],\n",
        "                          [dense_out, gru_state_out])"
      ],
      "metadata": {
        "id": "KADjUN-IOIC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the encoder's embedding before its updated by decoder for later\n",
        "# optional\n",
        "original_body_encoding = body_encoding"
      ],
      "metadata": {
        "id": "pYLMoRzCOH_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_value = np.array(tok2.word_index['_start_']).reshape(1, 1)"
      ],
      "metadata": {
        "id": "ig6b9TK2OH8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzfcWl2nOH05",
        "outputId": "78ff90c6-16b9-48cb-dfd0-35e16bf9e5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1]])"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence = []\n",
        "stop_condition = False"
      ],
      "metadata": {
        "id": "96HWjjzkOHx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_inv = dict((v, k) for k, v in tok2.word_index.items())\n",
        "vocabulary_inv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trq4uxktOG7w",
        "outputId": "61d5f78a-fbb7-490b-a492-61870627cc9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: '_start_',\n",
              " 2: '_end_',\n",
              " 3: 'great',\n",
              " 4: 'the',\n",
              " 5: 'good',\n",
              " 6: 'for',\n",
              " 7: 'a',\n",
              " 8: 'and',\n",
              " 9: 'best',\n",
              " 10: 'not',\n",
              " 11: 'my',\n",
              " 12: 'love',\n",
              " 13: 'this',\n",
              " 14: 'it',\n",
              " 15: 'coffee',\n",
              " 16: 'but',\n",
              " 17: 'i',\n",
              " 18: 'of',\n",
              " 19: 'tea',\n",
              " 20: 'to',\n",
              " 21: 'is',\n",
              " 22: 'taste',\n",
              " 23: 'in',\n",
              " 24: 'these',\n",
              " 25: 'like',\n",
              " 26: 'product',\n",
              " 27: 'dog',\n",
              " 28: 'very',\n",
              " 29: 'price',\n",
              " 30: 'flavor',\n",
              " 31: 'ever',\n",
              " 32: 'food',\n",
              " 33: 'delicious',\n",
              " 34: 'as',\n",
              " 35: 'you',\n",
              " 36: 'loves',\n",
              " 37: 'with',\n",
              " 38: 'favorite',\n",
              " 39: 'too',\n",
              " 40: 'are',\n",
              " 41: 'dogs',\n",
              " 42: 'on',\n",
              " 43: 'tasty',\n",
              " 44: 'excellent',\n",
              " 45: 'better',\n",
              " 46: 'so',\n",
              " 47: 'snack',\n",
              " 48: 'healthy',\n",
              " 49: 'them',\n",
              " 50: 'just',\n",
              " 51: 'chocolate',\n",
              " 52: 'than',\n",
              " 53: 'what',\n",
              " 54: 'at',\n",
              " 55: 'free',\n",
              " 56: 'no',\n",
              " 57: 'one',\n",
              " 58: 'yummy',\n",
              " 59: 'tastes',\n",
              " 60: 'hot',\n",
              " 61: 'tasting',\n",
              " 62: 'stuff',\n",
              " 63: 'all',\n",
              " 64: 'chips',\n",
              " 65: 'cats',\n",
              " 66: 'cat',\n",
              " 67: 'really',\n",
              " 68: 'treat',\n",
              " 69: 'nice',\n",
              " 70: 'sweet',\n",
              " 71: 'have',\n",
              " 72: 'its',\n",
              " 73: 'little',\n",
              " 74: 'me',\n",
              " 75: 'buy',\n",
              " 76: 'quality',\n",
              " 77: 'perfect',\n",
              " 78: 'your',\n",
              " 79: 'treats',\n",
              " 80: 'from',\n",
              " 81: 'be',\n",
              " 82: 'dont',\n",
              " 83: 'sugar',\n",
              " 84: 'green',\n",
              " 85: 'mix',\n",
              " 86: 'organic',\n",
              " 87: 'if',\n",
              " 88: 'easy',\n",
              " 89: 'much',\n",
              " 90: 'cup',\n",
              " 91: 'bad',\n",
              " 92: 'cookies',\n",
              " 93: 'was',\n",
              " 94: 'that',\n",
              " 95: 'more',\n",
              " 96: 'they',\n",
              " 97: 'gluten',\n",
              " 98: 'way',\n",
              " 99: 'wonderful',\n",
              " 100: 'only',\n",
              " 101: 'out',\n",
              " 102: 'awesome',\n",
              " 103: 'strong',\n",
              " 104: 'can',\n",
              " 105: 'get',\n",
              " 106: 'candy',\n",
              " 107: 'amazon',\n",
              " 108: 'salt',\n",
              " 109: 'value',\n",
              " 110: 'our',\n",
              " 111: 'cant',\n",
              " 112: 'or',\n",
              " 113: 'drink',\n",
              " 114: 'sauce',\n",
              " 115: 'cereal',\n",
              " 116: 'bars',\n",
              " 117: 'ive',\n",
              " 118: 'an',\n",
              " 119: 'fresh',\n",
              " 120: 'oil',\n",
              " 121: 'eat',\n",
              " 122: 'up',\n",
              " 123: 'real',\n",
              " 124: 'made',\n",
              " 125: 'bar',\n",
              " 126: 'will',\n",
              " 127: 'hard',\n",
              " 128: 'do',\n",
              " 129: 'find',\n",
              " 130: 'had',\n",
              " 131: 'ok',\n",
              " 132: 'coconut',\n",
              " 133: 'go',\n",
              " 134: 'baby',\n",
              " 135: 'decaf',\n",
              " 136: 'kcup',\n",
              " 137: 'low',\n",
              " 138: 'time',\n",
              " 139: 'bold',\n",
              " 140: 'water',\n",
              " 141: 'kids',\n",
              " 142: 'popcorn',\n",
              " 143: 'breakfast',\n",
              " 144: 'high',\n",
              " 145: 'happy',\n",
              " 146: 'new',\n",
              " 147: 'there',\n",
              " 148: 'dark',\n",
              " 149: 'butter',\n",
              " 150: 'expensive',\n",
              " 151: 'worth',\n",
              " 152: 'deal',\n",
              " 153: 'blend',\n",
              " 154: 'pretty',\n",
              " 155: 'amazing',\n",
              " 156: 'works',\n",
              " 157: 'yum',\n",
              " 158: 'expected',\n",
              " 159: 'use',\n",
              " 160: 'vanilla',\n",
              " 161: 'old',\n",
              " 162: 'by',\n",
              " 163: 'makes',\n",
              " 164: 'natural',\n",
              " 165: 'loved',\n",
              " 166: 'rice',\n",
              " 167: 'kcups',\n",
              " 168: 'money',\n",
              " 169: 'has',\n",
              " 170: 'gift',\n",
              " 171: 'shipping',\n",
              " 172: 'again',\n",
              " 173: 'chicken',\n",
              " 174: 'service',\n",
              " 175: 'doesnt',\n",
              " 176: 'smooth',\n",
              " 177: 'we',\n",
              " 178: 'alternative',\n",
              " 179: 'big',\n",
              " 180: 'fast',\n",
              " 181: 'soup',\n",
              " 182: 'peanut',\n",
              " 183: 'jerky',\n",
              " 184: 'does',\n",
              " 185: 'roast',\n",
              " 186: 'cookie',\n",
              " 187: 'pack',\n",
              " 188: 'light',\n",
              " 189: 'protein',\n",
              " 190: 'bread',\n",
              " 191: 'small',\n",
              " 192: 'quick',\n",
              " 193: 'without',\n",
              " 194: 'black',\n",
              " 195: 'well',\n",
              " 196: 'some',\n",
              " 197: 'even',\n",
              " 198: 'pasta',\n",
              " 199: 'review',\n",
              " 200: 'would',\n",
              " 201: 'far',\n",
              " 202: 'milk',\n",
              " 203: 'instant',\n",
              " 204: 'other',\n",
              " 205: 'packaging',\n",
              " 206: 'k',\n",
              " 207: 'chai',\n",
              " 208: 'item',\n",
              " 209: 'found',\n",
              " 210: 'flavored',\n",
              " 211: 'didnt',\n",
              " 212: 'make',\n",
              " 213: 'syrup',\n",
              " 214: 'fantastic',\n",
              " 215: 'enough',\n",
              " 216: 'bit',\n",
              " 217: 'convenient',\n",
              " 218: 'energy',\n",
              " 219: 'fruit',\n",
              " 220: 'right',\n",
              " 221: 'cocoa',\n",
              " 222: 'thing',\n",
              " 223: 'spicy',\n",
              " 224: 'beans',\n",
              " 225: 'dry',\n",
              " 226: 'ginger',\n",
              " 227: 'bag',\n",
              " 228: 'cheese',\n",
              " 229: 'about',\n",
              " 230: 'salty',\n",
              " 231: 'work',\n",
              " 232: 'when',\n",
              " 233: 'super',\n",
              " 234: 'chip',\n",
              " 235: 'now',\n",
              " 236: 'diet',\n",
              " 237: 'spice',\n",
              " 238: 'bags',\n",
              " 239: 'rich',\n",
              " 240: '5',\n",
              " 241: 'order',\n",
              " 242: 'crunchy',\n",
              " 243: 'whole',\n",
              " 244: 'keurig',\n",
              " 245: 'were',\n",
              " 246: 'red',\n",
              " 247: 'gum',\n",
              " 248: 'honey',\n",
              " 249: 'likes',\n",
              " 250: 'most',\n",
              " 251: 'never',\n",
              " 252: 'beef',\n",
              " 253: 'must',\n",
              " 254: 'first',\n",
              " 255: 'wow',\n",
              " 256: 'kind',\n",
              " 257: 'box',\n",
              " 258: 'same',\n",
              " 259: 'tasted',\n",
              " 260: 'weak',\n",
              " 261: '2',\n",
              " 262: 'size',\n",
              " 263: 'could',\n",
              " 264: 'still',\n",
              " 265: 'oatmeal',\n",
              " 266: 'cups',\n",
              " 267: 'fat',\n",
              " 268: 'nothing',\n",
              " 269: 'flavors',\n",
              " 270: 'potato',\n",
              " 271: 'day',\n",
              " 272: 'try',\n",
              " 273: 'brand',\n",
              " 274: 'long',\n",
              " 275: 'tried',\n",
              " 276: 'market',\n",
              " 277: 'morning',\n",
              " 278: 'bitter',\n",
              " 279: 'every',\n",
              " 280: 'around',\n",
              " 281: 'used',\n",
              " 282: 'nuts',\n",
              " 283: 'lovers',\n",
              " 284: 'cheaper',\n",
              " 285: 'store',\n",
              " 286: 'training',\n",
              " 287: 'over',\n",
              " 288: 'less',\n",
              " 289: 'im',\n",
              " 290: 'did',\n",
              " 291: 'crackers',\n",
              " 292: 'regular',\n",
              " 293: 'french',\n",
              " 294: 'mountain',\n",
              " 295: 'got',\n",
              " 296: 'white',\n",
              " 297: 'wrong',\n",
              " 298: 'always',\n",
              " 299: 'puppy',\n",
              " 300: 'starbucks',\n",
              " 301: 'flavorful',\n",
              " 302: 'china',\n",
              " 303: 'after',\n",
              " 304: 'back',\n",
              " 305: 'why',\n",
              " 306: 'corn',\n",
              " 307: 'world',\n",
              " 308: 'wont',\n",
              " 309: 'pricey',\n",
              " 310: 'lemon',\n",
              " 311: 'substitute',\n",
              " 312: 'any',\n",
              " 313: 'stale',\n",
              " 314: 'granola',\n",
              " 315: 'fun',\n",
              " 316: 'delivery',\n",
              " 317: 'choice',\n",
              " 318: 'horrible',\n",
              " 319: 'cinnamon',\n",
              " 320: 'picky',\n",
              " 321: 'beware',\n",
              " 322: 'gf',\n",
              " 323: 'another',\n",
              " 324: 'need',\n",
              " 325: 'fine',\n",
              " 326: 'powder',\n",
              " 327: 'ingredients',\n",
              " 328: 'iced',\n",
              " 329: 'meal',\n",
              " 330: 'different',\n",
              " 331: 'em',\n",
              " 332: 'full',\n",
              " 333: 'poor',\n",
              " 334: 'how',\n",
              " 335: 'decent',\n",
              " 336: 'variety',\n",
              " 337: 'greenies',\n",
              " 338: 'texture',\n",
              " 339: 'finally',\n",
              " 340: 'worst',\n",
              " 341: 'seasoning',\n",
              " 342: 'house',\n",
              " 343: 'terrible',\n",
              " 344: 'stash',\n",
              " 345: 'formula',\n",
              " 346: 'espresso',\n",
              " 347: 'soft',\n",
              " 348: 'glutenfree',\n",
              " 349: 'should',\n",
              " 350: 'down',\n",
              " 351: 'licorice',\n",
              " 352: 'pods',\n",
              " 353: 'refreshing',\n",
              " 354: 'who',\n",
              " 355: 'simply',\n",
              " 356: 'last',\n",
              " 357: 'home',\n",
              " 358: 'smells',\n",
              " 359: 'cans',\n",
              " 360: 'hazelnut',\n",
              " 361: 'waste',\n",
              " 362: 'stars',\n",
              " 363: 'cake',\n",
              " 364: 'smell',\n",
              " 365: 'canned',\n",
              " 366: 'almonds',\n",
              " 367: 'yet',\n",
              " 368: 'okay',\n",
              " 369: 'available',\n",
              " 370: 'juice',\n",
              " 371: 'blue',\n",
              " 372: 'pop',\n",
              " 373: 'here',\n",
              " 374: 'pretzels',\n",
              " 375: 'health',\n",
              " 376: 'crazy',\n",
              " 377: 'exactly',\n",
              " 378: 'family',\n",
              " 379: 'cream',\n",
              " 380: 'original',\n",
              " 381: 'many',\n",
              " 382: 'almost',\n",
              " 383: 'extra',\n",
              " 384: 'crunch',\n",
              " 385: 'purchase',\n",
              " 386: 'chews',\n",
              " 387: 'olive',\n",
              " 388: 'want',\n",
              " 389: 'absolutely',\n",
              " 390: 'son',\n",
              " 391: 'wheat',\n",
              " 392: 'sure',\n",
              " 393: 'say',\n",
              " 394: '1',\n",
              " 395: 'nutritious',\n",
              " 396: '3',\n",
              " 397: 'disappointed',\n",
              " 398: 'dried',\n",
              " 399: 'lot',\n",
              " 400: 'off',\n",
              " 401: 'mild',\n",
              " 402: 'oh',\n",
              " 403: 'senseo',\n",
              " 404: 'liked',\n",
              " 405: 'date',\n",
              " 406: 'apple',\n",
              " 407: 'addictive',\n",
              " 408: 'chew',\n",
              " 409: 'know',\n",
              " 410: 'own',\n",
              " 411: '4',\n",
              " 412: 'snacks',\n",
              " 413: 'recommended',\n",
              " 414: 'overpriced',\n",
              " 415: 'eating',\n",
              " 416: 'grey',\n",
              " 417: 'bought',\n",
              " 418: 'pure',\n",
              " 419: 'earl',\n",
              " 420: 'wish',\n",
              " 421: 'hit',\n",
              " 422: 'calories',\n",
              " 423: 'nut',\n",
              " 424: 'quite',\n",
              " 425: 'says',\n",
              " 426: 'people',\n",
              " 427: 'flour',\n",
              " 428: 'brown',\n",
              " 429: 'baking',\n",
              " 430: 'think',\n",
              " 431: 'cheap',\n",
              " 432: 'heaven',\n",
              " 433: 'looking',\n",
              " 434: 'life',\n",
              " 435: 'option',\n",
              " 436: 'package',\n",
              " 437: 'things',\n",
              " 438: 'please',\n",
              " 439: 'idea',\n",
              " 440: 'special',\n",
              " 441: 'almond',\n",
              " 442: 'lots',\n",
              " 443: 'ordered',\n",
              " 444: 'filling',\n",
              " 445: 'give',\n",
              " 446: 'two',\n",
              " 447: 'chewy',\n",
              " 448: 'been',\n",
              " 449: 'chili',\n",
              " 450: 'customer',\n",
              " 451: 'calorie',\n",
              " 452: 'soda',\n",
              " 453: 'toy',\n",
              " 454: 'am',\n",
              " 455: 'bbq',\n",
              " 456: 'seeds',\n",
              " 457: 'ice',\n",
              " 458: 'orange',\n",
              " 459: 'carb',\n",
              " 460: 'arrived',\n",
              " 461: 'vinegar',\n",
              " 462: 'sour',\n",
              " 463: 'mint',\n",
              " 464: 'needs',\n",
              " 465: 'herbal',\n",
              " 466: 'products',\n",
              " 467: 'fan',\n",
              " 468: 'oz',\n",
              " 469: 'bean',\n",
              " 470: 'those',\n",
              " 471: 'soy',\n",
              " 472: 'yes',\n",
              " 473: 'pumpkin',\n",
              " 474: 'awful',\n",
              " 475: 'company',\n",
              " 476: 'italian',\n",
              " 477: 'favorites',\n",
              " 478: 'fabulous',\n",
              " 479: 'pepper',\n",
              " 480: 'save',\n",
              " 481: 'noodles',\n",
              " 482: '6',\n",
              " 483: 'something',\n",
              " 484: 'wellness',\n",
              " 485: 'dressing',\n",
              " 486: 'maple',\n",
              " 487: 'us',\n",
              " 488: 'raw',\n",
              " 489: '12',\n",
              " 490: 'weight',\n",
              " 491: 'highly',\n",
              " 492: 'teeth',\n",
              " 493: 'their',\n",
              " 494: 'raspberry',\n",
              " 495: '100',\n",
              " 496: 'brew',\n",
              " 497: 'cold',\n",
              " 498: 'name',\n",
              " 499: 'received',\n",
              " 500: 'aroma',\n",
              " 501: 'advertised',\n",
              " 502: 'salmon',\n",
              " 503: 'grain',\n",
              " 504: 'kitty',\n",
              " 505: 'top',\n",
              " 506: 'thought',\n",
              " 507: 'fiber',\n",
              " 508: 'wanted',\n",
              " 509: 'baked',\n",
              " 510: 'teas',\n",
              " 511: 'newmans',\n",
              " 512: 'daughter',\n",
              " 513: 'goodness',\n",
              " 514: 'stores',\n",
              " 515: 'helps',\n",
              " 516: 'though',\n",
              " 517: 'jelly',\n",
              " 518: 'priced',\n",
              " 519: 'everything',\n",
              " 520: 'definitely',\n",
              " 521: 'cherry',\n",
              " 522: 'before',\n",
              " 523: 'gourmet',\n",
              " 524: 'close',\n",
              " 525: 'bottle',\n",
              " 526: 'tuna',\n",
              " 527: 'where',\n",
              " 528: 'cooking',\n",
              " 529: 'fish',\n",
              " 530: 'creamy',\n",
              " 531: 'star',\n",
              " 532: 'mustard',\n",
              " 533: 'away',\n",
              " 534: 'sea',\n",
              " 535: 'goes',\n",
              " 536: 'artificial',\n",
              " 537: 'keeps',\n",
              " 538: 'simple',\n",
              " 539: 'husband',\n",
              " 540: 'plus',\n",
              " 541: 'years',\n",
              " 542: 'recipe',\n",
              " 543: '10',\n",
              " 544: 'buying',\n",
              " 545: 'greatest',\n",
              " 546: 'keep',\n",
              " 547: 'salad',\n",
              " 548: 'sticks',\n",
              " 549: 'bland',\n",
              " 550: 'cost',\n",
              " 551: 'large',\n",
              " 552: 'mini',\n",
              " 553: 'else',\n",
              " 554: 'premium',\n",
              " 555: 'english',\n",
              " 556: 'broken',\n",
              " 557: 'peach',\n",
              " 558: 'change',\n",
              " 559: 'meat',\n",
              " 560: 'anything',\n",
              " 561: 'live',\n",
              " 562: 'replacement',\n",
              " 563: 'nom',\n",
              " 564: 'medium',\n",
              " 565: 'gone',\n",
              " 566: 'hair',\n",
              " 567: 'kick',\n",
              " 568: 'expiration',\n",
              " 569: 'cracker',\n",
              " 570: 'stevia',\n",
              " 571: 'sodium',\n",
              " 572: 'bones',\n",
              " 573: 'add',\n",
              " 574: 'clean',\n",
              " 575: 'rocks',\n",
              " 576: 'caramel',\n",
              " 577: 'hate',\n",
              " 578: 'foods',\n",
              " 579: 'satisfying',\n",
              " 580: 'earth',\n",
              " 581: 'year',\n",
              " 582: 'cappuccino',\n",
              " 583: 'wild',\n",
              " 584: 'true',\n",
              " 585: 'allergies',\n",
              " 586: 'bulk',\n",
              " 587: 'half',\n",
              " 588: 'ones',\n",
              " 589: 'kid',\n",
              " 590: 'everyone',\n",
              " 591: 'believe',\n",
              " 592: 'needed',\n",
              " 593: 'making',\n",
              " 594: 'fuel',\n",
              " 595: 'maybe',\n",
              " 596: 'then',\n",
              " 597: 'youll',\n",
              " 598: 'glad',\n",
              " 599: 'magic',\n",
              " 600: 'actually',\n",
              " 601: 'mom',\n",
              " 602: 'lime',\n",
              " 603: 'contains',\n",
              " 604: 'wouldnt',\n",
              " 605: 'leaf',\n",
              " 606: 'wife',\n",
              " 607: 'thats',\n",
              " 608: 'ingredient',\n",
              " 609: 'sweetener',\n",
              " 610: 'changed',\n",
              " 611: 'usa',\n",
              " 612: 'care',\n",
              " 613: 'kettle',\n",
              " 614: 'bite',\n",
              " 615: 'pill',\n",
              " 616: 'dented',\n",
              " 617: 'nutrition',\n",
              " 618: 'seems',\n",
              " 619: 'caffeine',\n",
              " 620: 'popchips',\n",
              " 621: 'thank',\n",
              " 622: 'terrific',\n",
              " 623: 'pleasant',\n",
              " 624: 'smaller',\n",
              " 625: 'pleased',\n",
              " 626: 'beautiful',\n",
              " 627: 'homemade',\n",
              " 628: 'enjoy',\n",
              " 629: 'addicted',\n",
              " 630: 'lover',\n",
              " 631: 'whats',\n",
              " 632: 'aftertaste',\n",
              " 633: 'vegan',\n",
              " 634: 'added',\n",
              " 635: 'packs',\n",
              " 636: 'take',\n",
              " 637: 'pet',\n",
              " 638: 'unique',\n",
              " 639: 'seller',\n",
              " 640: 'version',\n",
              " 641: 'look',\n",
              " 642: 'jet',\n",
              " 643: 'miss',\n",
              " 644: 'theyre',\n",
              " 645: 'seed',\n",
              " 646: 'stop',\n",
              " 647: 'pie',\n",
              " 648: 'touch',\n",
              " 649: 'grocery',\n",
              " 650: 'help',\n",
              " 651: 'job',\n",
              " 652: 'may',\n",
              " 653: 'movie',\n",
              " 654: 'local',\n",
              " 655: 'nasty',\n",
              " 656: 'double',\n",
              " 657: 'delight',\n",
              " 658: 'wine',\n",
              " 659: 'zukes',\n",
              " 660: 'per',\n",
              " 661: 'outstanding',\n",
              " 662: 'plain',\n",
              " 663: 'warning',\n",
              " 664: 'summer',\n",
              " 665: 'peppermint',\n",
              " 666: 'blueberry',\n",
              " 667: 'looks',\n",
              " 668: 'garlic',\n",
              " 669: 'going',\n",
              " 670: 'misleading',\n",
              " 671: 'gummy',\n",
              " 672: 'rock',\n",
              " 673: 'lunch',\n",
              " 674: 'pieces',\n",
              " 675: 'bacon',\n",
              " 676: 'thin',\n",
              " 677: 'source',\n",
              " 678: 'crispy',\n",
              " 679: 'plastic',\n",
              " 680: 'feel',\n",
              " 681: 'winner',\n",
              " 682: 'ground',\n",
              " 683: 'healthier',\n",
              " 684: 'dental',\n",
              " 685: 'gets',\n",
              " 686: 'toddler',\n",
              " 687: 'peanuts',\n",
              " 688: 'three',\n",
              " 689: 'she',\n",
              " 690: 'read',\n",
              " 691: 'isnt',\n",
              " 692: 'description',\n",
              " 693: 'described',\n",
              " 694: 'youre',\n",
              " 695: 'puck',\n",
              " 696: 'couldnt',\n",
              " 697: 'tullys',\n",
              " 698: 'reviews',\n",
              " 699: 'wheres',\n",
              " 700: 'bears',\n",
              " 701: 'sugarfree',\n",
              " 702: 'tiny',\n",
              " 703: 'lovely',\n",
              " 704: 'anywhere',\n",
              " 705: 'machine',\n",
              " 706: 'doggie',\n",
              " 707: 'boxes',\n",
              " 708: 'coffees',\n",
              " 709: 'others',\n",
              " 710: 'golden',\n",
              " 711: 'classic',\n",
              " 712: 'come',\n",
              " 713: 'lipton',\n",
              " 714: 'picture',\n",
              " 715: 'gross',\n",
              " 716: 'mouth',\n",
              " 717: 'huge',\n",
              " 718: 'hands',\n",
              " 719: 'tummy',\n",
              " 720: 'bargain',\n",
              " 721: 'start',\n",
              " 722: 'grove',\n",
              " 723: 'banana',\n",
              " 724: 'pizza',\n",
              " 725: 'bone',\n",
              " 726: 'month',\n",
              " 727: 'kit',\n",
              " 728: 'eater',\n",
              " 729: 'yuck',\n",
              " 730: 'her',\n",
              " 731: 'omg',\n",
              " 732: 'gloria',\n",
              " 733: 'strawberry',\n",
              " 734: 'party',\n",
              " 735: 'wolfgang',\n",
              " 736: 'babies',\n",
              " 737: 'everyday',\n",
              " 738: 'pay',\n",
              " 739: 'heat',\n",
              " 740: 'mill',\n",
              " 741: 'n',\n",
              " 742: 'pockets',\n",
              " 743: 'pancake',\n",
              " 744: 'solid',\n",
              " 745: 'biscuits',\n",
              " 746: '8',\n",
              " 747: 'extract',\n",
              " 748: 'gold',\n",
              " 749: 'mocha',\n",
              " 750: 'tough',\n",
              " 751: 'curry',\n",
              " 752: 'cider',\n",
              " 753: 'latte',\n",
              " 754: 'totally',\n",
              " 755: 'square',\n",
              " 756: 'kona',\n",
              " 757: 'steak',\n",
              " 758: 'control',\n",
              " 759: 'bobs',\n",
              " 760: 'dessert',\n",
              " 761: 'problem',\n",
              " 762: 'sick',\n",
              " 763: 'watch',\n",
              " 764: 'addition',\n",
              " 765: 'roasted',\n",
              " 766: 'mints',\n",
              " 767: 'berry',\n",
              " 768: 'reasonable',\n",
              " 769: 'stomach',\n",
              " 770: 'dr',\n",
              " 771: 'recommend',\n",
              " 772: 'tart',\n",
              " 773: 'few',\n",
              " 774: 'into',\n",
              " 775: 'single',\n",
              " 776: 'difference',\n",
              " 777: 'prefer',\n",
              " 778: 'pod',\n",
              " 779: 'staple',\n",
              " 780: 'instead',\n",
              " 781: 'mango',\n",
              " 782: 'handy',\n",
              " 783: 'slim',\n",
              " 784: 'dinner',\n",
              " 785: 'mac',\n",
              " 786: 'sent',\n",
              " 787: 'o',\n",
              " 788: 'absolute',\n",
              " 789: 'remember',\n",
              " 790: 'average',\n",
              " 791: 'jack',\n",
              " 792: 'count',\n",
              " 793: 'worked',\n",
              " 794: 'cut',\n",
              " 795: 'pancakes',\n",
              " 796: 'days',\n",
              " 797: 'getting',\n",
              " 798: 'side',\n",
              " 799: 'noodle',\n",
              " 800: 'tomato',\n",
              " 801: 'second',\n",
              " 802: 'bran',\n",
              " 803: 'truly',\n",
              " 804: 'joe',\n",
              " 805: 'quaker',\n",
              " 806: 'jeans',\n",
              " 807: 'crack',\n",
              " 808: 'nutiva',\n",
              " 809: 'pork',\n",
              " 810: 'came',\n",
              " 811: 'packaged',\n",
              " 812: 'plant',\n",
              " 813: 'brands',\n",
              " 814: 'diamond',\n",
              " 815: 'cute',\n",
              " 816: 'prices',\n",
              " 817: 'convenience',\n",
              " 818: 'crystal',\n",
              " 819: 'style',\n",
              " 820: 'paste',\n",
              " 821: 'sensitive',\n",
              " 822: 'oats',\n",
              " 823: 'bay',\n",
              " 824: 'gas',\n",
              " 825: 'affordable',\n",
              " 826: 'cherries',\n",
              " 827: 'drinking',\n",
              " 828: 'amount',\n",
              " 829: 'crisp',\n",
              " 830: 'versatile',\n",
              " 831: 'expect',\n",
              " 832: 'gotta',\n",
              " 833: 'longer',\n",
              " 834: 'loose',\n",
              " 835: 'turkey',\n",
              " 836: 'cafe',\n",
              " 837: 'planet',\n",
              " 838: 'veggie',\n",
              " 839: 'splenda',\n",
              " 840: 'jalapeno',\n",
              " 841: 'jar',\n",
              " 842: 'tree',\n",
              " 843: 'smart',\n",
              " 844: 'fair',\n",
              " 845: 'boost',\n",
              " 846: 'belly',\n",
              " 847: 'creme',\n",
              " 848: 'twinings',\n",
              " 849: 'thanks',\n",
              " 850: 'calm',\n",
              " 851: 'mixed',\n",
              " 852: 'skin',\n",
              " 853: 'mess',\n",
              " 854: 'disappointing',\n",
              " 855: 'timothys',\n",
              " 856: 'increase',\n",
              " 857: 'darn',\n",
              " 858: 'short',\n",
              " 859: 'being',\n",
              " 860: 'both',\n",
              " 861: 'subscribe',\n",
              " 862: 'liver',\n",
              " 863: 'open',\n",
              " 864: 'liquid',\n",
              " 865: 'emerils',\n",
              " 866: 'shipped',\n",
              " 867: 'tangy',\n",
              " 868: 'experience',\n",
              " 869: 'pamelas',\n",
              " 870: 'surprise',\n",
              " 871: 'drinks',\n",
              " 872: 'amazoncom',\n",
              " 873: 'finicky',\n",
              " 874: 'nutty',\n",
              " 875: 'slightly',\n",
              " 876: 'salsa',\n",
              " 877: 'effective',\n",
              " 878: 'coffe',\n",
              " 879: 'daily',\n",
              " 880: 'once',\n",
              " 881: 'night',\n",
              " 882: 'truffle',\n",
              " 883: 'he',\n",
              " 884: 'toffee',\n",
              " 885: 'five',\n",
              " 886: 'months',\n",
              " 887: 'beat',\n",
              " 888: 'tortilla',\n",
              " 889: '50',\n",
              " 890: 'christmas',\n",
              " 891: 'fruity',\n",
              " 892: 'also',\n",
              " 893: 'beer',\n",
              " 894: 'fix',\n",
              " 895: 'gave',\n",
              " 896: 'stick',\n",
              " 897: 'stinky',\n",
              " 898: 'safe',\n",
              " 899: 'chewers',\n",
              " 900: 'san',\n",
              " 901: 'chocolates',\n",
              " 902: 'msg',\n",
              " 903: 'cook',\n",
              " 904: 'kitchen',\n",
              " 905: 'hoped',\n",
              " 906: 'office',\n",
              " 907: 'pictured',\n",
              " 908: 'delish',\n",
              " 909: 'leaves',\n",
              " 910: 'thai',\n",
              " 911: 'authentic',\n",
              " 912: 'lavazza',\n",
              " 913: 'kitties',\n",
              " 914: 'friend',\n",
              " 915: 'wake',\n",
              " 916: 'problems',\n",
              " 917: 'cheddar',\n",
              " 918: 'maker',\n",
              " 919: 'id',\n",
              " 920: 'weird',\n",
              " 921: 'extremely',\n",
              " 922: 'watery',\n",
              " 923: 'tell',\n",
              " 924: 'beverage',\n",
              " 925: 'quantity',\n",
              " 926: 'hint',\n",
              " 927: 'ounce',\n",
              " 928: 'sampler',\n",
              " 929: 'older',\n",
              " 930: 'least',\n",
              " 931: 'purchased',\n",
              " 932: 'friendly',\n",
              " 933: 'disgusting',\n",
              " 934: 'robust',\n",
              " 935: 'quickly',\n",
              " 936: 'eats',\n",
              " 937: 'grass',\n",
              " 938: 'power',\n",
              " 939: 'messy',\n",
              " 940: 'shop',\n",
              " 941: 'hips',\n",
              " 942: 'pecan',\n",
              " 943: 'jasmine',\n",
              " 944: 'wait',\n",
              " 945: 'busy',\n",
              " 946: 'gummi',\n",
              " 947: 'online',\n",
              " 948: 'color',\n",
              " 949: 'country',\n",
              " 950: 'donut',\n",
              " 951: 'candies',\n",
              " 952: 'breath',\n",
              " 953: 'while',\n",
              " 954: 'wasnt',\n",
              " 955: 'impressed',\n",
              " 956: 'said',\n",
              " 957: 'sesame',\n",
              " 958: 'his',\n",
              " 959: 'delivered',\n",
              " 960: 'thick',\n",
              " 961: 'seem',\n",
              " 962: 'packages',\n",
              " 963: 'flakes',\n",
              " 964: 'bed',\n",
              " 965: 'melted',\n",
              " 966: 'eaten',\n",
              " 967: 'check',\n",
              " 968: 'ordering',\n",
              " 969: 'weve',\n",
              " 970: 'paws',\n",
              " 971: 'container',\n",
              " 972: 'expired',\n",
              " 973: 'anyone',\n",
              " 974: 'gravy',\n",
              " 975: 'addicting',\n",
              " 976: 'missing',\n",
              " 977: 'see',\n",
              " 978: 'cool',\n",
              " 979: 'acid',\n",
              " 980: 'put',\n",
              " 981: 'delightful',\n",
              " 982: 'nestle',\n",
              " 983: 'virgin',\n",
              " 984: 'gives',\n",
              " 985: 'kidding',\n",
              " 986: 'cal',\n",
              " 987: 'bring',\n",
              " 988: 'ramen',\n",
              " 989: '24',\n",
              " 990: 'dairy',\n",
              " 991: 'lasting',\n",
              " 992: 'zero',\n",
              " 993: 'superb',\n",
              " 994: 'yogurt',\n",
              " 995: 'careful',\n",
              " 996: 'body',\n",
              " 997: 'might',\n",
              " 998: 'cranberry',\n",
              " 999: 'rip',\n",
              " 1000: 'links',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while not stop_condition:\n",
        "    #print(1)\n",
        "    preds, st = decoder_model.predict([state_value, body_encoding])\n",
        "\n",
        "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
        "    pred_word_str = vocabulary_inv[pred_idx]\n",
        "    print(pred_word_str)\n",
        "    if pred_word_str == '_end_' or len(decoded_sentence) >= maxlen2:\n",
        "        stop_condition = True\n",
        "        break\n",
        "    decoded_sentence.append(pred_word_str)\n",
        "\n",
        "    # update the decoder for the next word\n",
        "    body_encoding = st\n",
        "    state_value = np.array(pred_idx).reshape(1, 1)\n",
        "    #print(state_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnV26Z5aOeuX",
        "outputId": "e5b0892a-ec27-433f-b757-d818a31f9802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "great\n",
            "product\n",
            "but\n",
            "_end_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compare to original summary\n",
        "\n",
        "print([test['summary_no_punctuation'][34]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8oe5YMYOeoF",
        "outputId": "dae5eaee-e878-45a2-8071-10efb33cdd61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['_start_ great for chicken curry dishes _end_']\n"
          ]
        }
      ]
    }
  ]
}